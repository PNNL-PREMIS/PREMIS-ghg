---
title: "Seconds to decades: soil respiration variability and sensitivity across temporal scales"
author: "Ben Bond-Lamberty"
date: "15 February 2019"
output: html_document
---

## CURRENT TODO LIST

* Insert NAs into continuous data for proper handling - use `complete` based on `year`, `month`, `day`
* Remove snowstorm day
* Investigate/remove screwy survey collar
* Compute mean difference between survey observations (X days)
* How often do we need to sample to get good monthly estimate? Test.
* Distirbutions in summary plot

## Introduction

* point 1
* point 2
* point 3
* point 4

To examine variability and sampling requirements at a variety of temporal scales, we combined (i) hourly, continuous measurements made by an 8-chamber IRGA, (ii) survey measurements made every ~10 days over a year at the same site, and (iii) data on annual fluxes from the SRDB. 

```{r setup, include=FALSE}
library(drake)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw())
library(lubridate)
library(kableExtra)
library(lattice)

readd("licor_data") %>% 
  filter(Group == "Control") %>% 
  arrange(Timestamp) %>% 
  mutate(Dest_Elevation = factor(Dest_Elevation,
                                 levels = c("Low", "Medium", "High"))) ->
  licor_data
readd("con_licor_data") %>% 
  # Filter for when system was in SALT plot (previously was testing)
  filter(Timestamp >= "2018-10-01") %>% 
  arrange(Timestamp, Port) ->
  cld

# Helper function - compute coefficient of variability (CV) between
# x[1] and x[2], x[2] and x[3], etc.
running_cv <- function(x) {
  out <- rep(NA_real_, length(x))
  for(i in seq_along(x)[-1]) {
    obs <- c(x[i-1], x[i])
    out[i] <- sd(obs) / mean(obs)
  }
  out
}
```

```{r filtering, echo=FALSE}
cld %>% 
  filter(Flux > 0,  # definitely a problem!
         R2 > 0.75  # probably a chamber closing problem otherwise
         # TODO: remove snowstorm day
  ) ->
  cld_clean
removed <- nrow(cld) - nrow(cld_clean)
```

## Methods

Continuous data: `r nrow (cld)` observations; `r removed` (`r round(removed / nrow(cld) * 100, 0)`%) removed because of chamber closure or other problems.

```{r continuous-plot, echo=FALSE}
ggplot(cld_clean, aes(Timestamp, Flux)) + geom_line() + facet_grid(Port ~ .)
```

Survey data: `r nrow (licor_data)` observations.

```{r survey-plot, echo=FALSE}
ggplot(licor_data, aes(Timestamp, Flux, group = Collar)) + 
  geom_point() + geom_line(color = "darkgrey") +
  facet_grid(~Dest_Elevation)
```


Time scales we're looking at:

* **Seconds to minutes**: how many times do we need to sample? Uses survey data. 
* **Hours**: what's correlation and CV between successive hours? Uses continuous data.
* **Days**: what's correlation and CV between successive days? Uses continuous data.
* **Months**: what's CV between successive months? How many times do we need to sample for a good annual estimate? Uses survey data.
* **Years**: What's the CV between successive years in the SRDB?
* **Years**: how much coverage is needed for robust annual estimate? This uses `Annual_coverage` field of SRDB. Not clear about this one.

## Results

### Seconds to minutes: variability

This uses the _survey_ data, examining the variability in multiple Licor observations separated by ~1 m versus 1, 2, 3...

```{r cv12, echo = FALSE}
licor_data %>% 
  group_by(Date, Group, Collar) %>%
  summarise(n = n(), meanFlux = mean(Flux), CV = sd(Flux) / mean(Flux)) %>% 
  filter(n == 2) ->
  meas_error_1

median_error_minutes <- median(meas_error_1$CV)
ggplot(meas_error_1, aes(x = CV)) + geom_histogram(bins = 20) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  geom_vline(xintercept = median_error_minutes, color = "red") +
  ylab("Count") + xlab("CV between successive IRGA measurements") +
  ggtitle(paste("N =", nrow(meas_error_1)))
```

The median measurement error here is ~`r round(median_error_minutes * 100, 0)`% for `r nrow(meas_error_1)` observations of fluxes between `r round(min(meas_error_1$meanFlux), 2)` and `r round(max(meas_error_1$meanFlux), 2)` µmol/m2/s.

### Seconds to minutes: sampling

How many times do we need to sample? Use the times Stephanie measured 2x, 3x, and 4x at collars (which we did early on to assess this question).

```{r seconds-to-minutes, echo = FALSE}
licor_data %>% 
  filter(Flux < 10) %>% 
  group_by(Date, Group, Collar) %>%
  summarize(n = n(), mean_gt_2 = mean(Flux), mean_1 = Flux[1], mean_2 = mean(Flux[1:2])) %>% 
  filter(n >= 3) -> 
  lds

lds %>% 
  gather(variable, value, mean_1, mean_2) ->
  lds_plot

ggplot(lds_plot, aes(x = mean_gt_2, y = value, color = variable)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm", linetype = 2) +
  geom_point() + 
  labs(x = "Mean flux of all (>=3) measurements", 
       y = "Mean flux of first 1 or 2 measurements")

lds %>% 
  gather(variable, value, mean_gt_2, mean_1, mean_2) %>% 
  mutate(variable = factor(variable, levels = c("mean_gt_2", "mean_2", "mean_1"))) ->
  lds_stats
m <- lm(value ~ variable, data = lds_stats)
summary(m)
TukeyHSD(aov(m))
```

Basically, you need two samples--one is not enough--but not more.

### Hours: variability

That's the correlation in the continuous time series? What's the CV between successive values?

```{r hours, echo=FALSE}
# Compute the partial autocorrelation function for each port (collar).
# First need to convert to a timeseries object so `pacf` can 'see'
# the gaps in the data
results <- list()
cld_cv <- list()
for(p in unique(cld_clean$Port)) {
  cld_p <- filter(cld_clean, Port == p)
  pacf_obj <- pacf(as.ts(cld_p$Flux,
                         start = min(cld_p$Timestamp),
                         end = max(cld_p$Timestamp)), 
                   na.action = na.pass,
                   plot = FALSE)
  results[[p]] <- tibble(Port = p,
                         lag = pacf_obj$lag[,,1], 
                         PACF = pacf_obj$acf[,,1])
  # Compute the CV between successive measurements
  cld_p$CV <- running_cv(cld_p$Flux)
  cld_p$T5_diff <- c(NA, diff(cld_p$T5))
  cld_p$SMoist_diff <- c(NA, diff(cld_p$SMoist))
  cld_cv[[p]] <- cld_p
}
# Combine results and compute mean PACF at each lag
bind_rows(results) %>% 
  group_by(lag) %>% 
  summarise(PACF_sd = sd(PACF), PACF = mean(PACF)) ->
  pacf_df
ggplot(pacf_df, aes(lag, PACF)) + 
  xlab("Lag (hours)") + geom_hline(yintercept = 0) +
  geom_point() + 
  geom_errorbar(aes(ymin = PACF - PACF_sd, ymax = PACF + PACF_sd)) + 
  geom_linerange(aes(ymin = 0, ymax = PACF), 
                 color = "darkgrey", linetype = 2)
```

In the figure above, Rs values measured in successive hours at a given collar exhibit a strong correlation (`r round(pacf_df$PACF[1], 3)`), and a moderate one at a two-hour lag (`r round(pacf_df$PACF[2], 3)`). Day-to-day (24 hour lag) observations are essentially uncorrelated (`r round(pacf_df$PACF[24], 3)`).

```{r, hours-cv, echo=FALSE}
cld_cv <- bind_rows(cld_cv) %>% 
  filter(!is.na(CV))
median_error_hours <- median(cld_cv$CV, na.rm = TRUE)

# Plot CV as it relates to changes in T5 and soil moisture, and test effects
xl <- c(-1, 1)
yl <- c(-0.02, 0.02)
cld_cv %>% 
  filter(T5_diff < min(xl) | T5_diff > max(xl) | 
           SMoist_diff < min(yl) | SMoist_diff > max(yl)) %>% 
  nrow ->
  excluded_count

ggplot(cld_cv, aes(T5_diff, SMoist_diff, color = CV)) +
  geom_point() + 
  coord_cartesian(xlim = xl, ylim = yl) +
  ggtitle(paste(excluded_count, "of", nrow(cld_cv), "observations not shown"))

summary(lm(CV ~ abs(T5_diff) * abs(SMoist_diff), data = cld_cv))

# Histogram of CV values
ggplot(cld_cv, aes(x = CV)) + geom_histogram(bins = 20) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  geom_vline(xintercept = median_error_hours, color = "red") +
  ylab("Count") + xlab("CV between successive hourly measurements") +
  ggtitle(paste("N =", nrow(cld_cv)))
```

The median measurement error here is ~`r round(median_error_hours * 100, 0)`% for `r nrow(cld_cv)` observations of fluxes between `r round(min(cld_cv$Flux), 2)` and `r round(max(cld_cv$Flux), 2)` µmol/m2/s.

Large hour-to-hour CV was significantly associated with changes in soil moisture!


### Days: variability

```{r days, echo=FALSE}
# Compute the partial autocorrelation function for daily mean of each
# port (collar).
results <- list()
cld_cv <- list()
for(p in unique(cld_clean$Port)) {
  cld_clean %>% 
    filter(Port == p) %>% 
    mutate(julian = julian(Timestamp)) %>% 
    group_by(day(Timestamp)) %>% 
    summarise(julian = mean(julian),
              Flux = mean(Flux)) ->
    cld_p
  pacf_obj <- pacf(as.ts(cld_p$Flux,
                         start = min(cld_p$julian),
                         end = max(cld_p$julian)), 
                   na.action = na.pass,
                   plot = FALSE)
  results[[p]] <- tibble(Port = p,
                         lag = pacf_obj$lag[,,1], 
                         PACF = pacf_obj$acf[,,1])
  # Compute the CV between successive measurements
  cld_p$CV <- running_cv(cld_p$Flux)
  cld_cv[[p]] <- cld_p
}
# Combine results and compute mean PACF at each lag
bind_rows(results) %>% 
  group_by(lag) %>% 
  summarise(PACF_sd = sd(PACF), PACF = mean(PACF)) ->
  pacf_df
ggplot(pacf_df, aes(lag, PACF)) + 
  xlab("Lag (days)") + geom_hline(yintercept = 0) +
  geom_point() + 
  geom_errorbar(aes(ymin = PACF - PACF_sd, ymax = PACF + PACF_sd)) + 
  geom_linerange(aes(ymin = 0, ymax = PACF), 
                 color = "darkgrey", linetype = 2)
```

In the figure above, Rs values measured in successive days at a given collar exhibit a strong correlation (`r round(pacf_df$PACF[1], 3)`), but no correlation after that.

```{r, days-cv, echo=FALSE}
cld_cv <- bind_rows(cld_cv)
median_error_days <- median(cld_cv$CV, na.rm = TRUE)
ggplot(cld_cv, aes(x = CV)) + geom_histogram(bins = 20) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  geom_vline(xintercept = median_error_hours, color = "red") +
  ylab("Count") + xlab("CV between successive daily measurements") +
  ggtitle(paste("N =", nrow(cld_cv)))
```

The median measurement error here is ~`r round(median_error_days * 100, 0)`% for `r nrow(cld_cv)` observations of fluxes between `r round(min(cld_cv$Flux), 2)` and `r round(max(cld_cv$Flux), 2)` µmol/m2/s.


### Months: variability

Plot the control survey data:

```{r months, echo=FALSE}
licor_data %>% 
  filter(Group == "Control") %>% 
  mutate(year = year(Timestamp), month = month(Timestamp)) %>% 
  group_by(Dest_Elevation, year, month, Collar) %>% 
  summarise(Timestamp = mean(Timestamp),
            Flux_sd = sd(Flux, na.rm = TRUE),
            Flux = mean(Flux, na.rm = TRUE)) %>% 
  ungroup %>% 
  arrange(Timestamp) ->
  ld_monthly
ggplot(ld_monthly, aes(Timestamp, Flux, group = Collar)) +
  geom_line() + 
  geom_ribbon(aes(ymin = Flux - Flux_sd, 
                  ymax = Flux + Flux_sd), alpha = I(0.2)) + 
  facet_wrap(~Dest_Elevation)
```

CV between successive measurements:

```{r months-cv, echo=FALSE}
ld_monthly %>% 
  group_by(Collar) %>% 
  mutate(CV = running_cv(Flux)) ->
  lmd_cv

median_cv_months <- median(lmd_cv$CV, na.rm = TRUE)
ggplot(lmd_cv, aes(x = CV)) + geom_histogram(bins = 20) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  geom_vline(xintercept = median_cv_months, color = "red") +
  ylab("Count") + xlab("CV between successive monthly measurements") +
  ggtitle(paste("N =", nrow(lmd_cv)))

```

Median CV is `r round(median_cv_months * 100, 0)`%.

How often do we need to sample to get good monthly number? (Use hourly data.)


### What's the SRDB interannual variability?

```{r srdb, echo=FALSE}
readd("srdb") %>% 
  mutate(CV = Rs_interann_err / Rs_annual) %>%
  filter(Leaf_habit == "Deciduous", Ecosystem_type == "Forest", !is.na(CV)) ->
  srdb
median_cv_annual <- median(srdb$CV, na.rm = TRUE)

ggplot(srdb, aes(x = CV)) + geom_histogram(bins = 20) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  geom_vline(xintercept = median_cv_annual, color = "red") +
  ylab("Count") + xlab("CV between successive years") +
  ggtitle(paste("N =", nrow(srdb)))
```

Median CV here is `r round(median_cv_annual * 100, 0)`%.


## Summary

```{r summary-graph, echo=FALSE}
spd <- 24 * 60 * 60
timescales <- c("1 minute" = 60,
                "1 hour" = 60 * 60,
                "1 day" = spd,
                "1 month" = spd * 30,
                "1 year" = spd * 365)
smry <- tibble(Seconds = timescales,
               Labels = names(timescales),
               CV = c(median_error_minutes, 
                      median_error_hours, 
                      median_error_days, 
                      median_cv_months, 
                      median_cv_annual))

distributions <- bind_rows(tibble(Labels = "1 minute", CV = meas_error_1$CV),
                           tibble(Labels = "1 hour", CV = cld_cv$CV),
                           tibble(Labels = "1 day", CV = cld_cv$CV),
                           tibble(Labels = "1 month", CV = lmd_cv$CV),
                           tibble(Labels = "1 year", CV = srdb$CV))

distributions$Seconds <- timescales[distributions$Labels]

ggplot(smry, aes(Seconds, CV, label = Labels, group = Labels)) +
  geom_violin(data = distributions, fill = NA, 
              draw_quantiles = c(0.9, 0.95), color = "darkgrey") +
  geom_point() + 
  geom_text(nudge_y = 0.05) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10 ^ x),
    labels = scales::trans_format("log10", scales::math_format(10 ^ .x))
  ) +
  coord_cartesian(ylim = c(0, 1)) +
  xlab("Timescale (seconds)")
```
