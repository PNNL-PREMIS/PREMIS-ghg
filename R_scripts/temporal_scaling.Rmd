---
title: "Seconds to decades: soil respiration variability and sensitivity across temporal scales"
author: "Ben Bond-Lamberty"
date: "11/17/2018"
output: html_document
---

```{r setup, include=FALSE}
library(drake)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw())
library(lubridate)
library(kableExtra)

licor_data <- readd("licor_data")
```

## Seconds to minutes

How many times do we need to sample? Uses multiple Licor observations versus 1, 2, 3...

```{r, echo = FALSE}
licor_data %>% 
  filter(Flux < 10) %>% 
  group_by(Date, Group, Collar) %>%
  summarize(n = n(), mean_gt_2 = mean(Flux), mean_1 = Flux[1], mean_2 = mean(Flux[1:2])) %>% 
  filter(n >= 3) -> 
  lds

lds %>% 
  gather(variable, value, mean_1, mean_2) ->
  lds_plot

ggplot(lds_plot, aes(x = mean_gt_2, y = value, color = variable)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm", linetype = 2) +
  geom_point() + 
  labs(x = "Mean flux of all (>=3) measurements", 
       y = "Mean flux of first 1 or 2 measurements")

lds %>% 
  gather(variable, value, mean_gt_2, mean_1, mean_2) %>% 
  mutate(variable = factor(variable, levels = c("mean_gt_2", "mean_2", "mean_1"))) ->
  lds_stats
m <- lm(value ~ variable, data = lds_stats)
summary(m)
TukeyHSD(aov(m))
```


* **Minutes to hours to days**: how frequently do we need to sample? Uses GCREW continuous data. How frequently do we need to sample?
* **Days to months**: how frequently do we need to sample for annual flux? Uses SERC survey data.
* **Years**: how much coverage is needed for robust annual estimate? This uses `Annual_coverage` field of SRDB. Not clear about this one.
* **Error introduced by RH versus RS**. Uses SRDB. Not sure about this. Hard.
* - our Nature anlysis assumed no measurement error. How could this be estimated?
* Random measurement error: "Random errors show up as different results for ostensibly the same repeated measurement. They can be estimated by comparing multiple measurements, and reduced by averaging multiple measurements." https://en.wikipedia.org/wiki/Observational_error#Random_errors_versus_systematic_errors
* - _continuous_ data: variability between measurement 1 and measurement 2
* - _survey_ (discontinuous) data: variability between m1 and m2 PLUS sampling error (sampling from continuous annual data)
* **Years to decades**: how well can we detect trends? This is Nature paper SI work.



### What's the CV between survey measurements 1 and 2?

```{r, echo = FALSE}
licor_data %>% 
  group_by(Date, Group, Collar) %>%
  summarise(n = n(), meanFlux = mean(Flux), cv = sd(Flux) / mean(Flux)) %>% 
  filter(n == 2) ->
  meas_error_1

ggplot(meas_error_1, aes(x = cv)) + geom_histogram(bins = 30) +
  scale_x_continuous(labels = scales::percent) +
  ggtitle("CV between successive Licor measurements")
median_error <- median(meas_error_1$cv)
```
OK, so the median measurement error here is ~`r round(median_error * 100, 0)`% for `r nrow(meas_error_1)` observations of fluxes between `r round(min(meas_error_1$meanFlux), 2)` and `r round(max(meas_error_1$meanFlux), 2)` Âµmol/m2/s.

### Nature paper analysis

```{r, echo=FALSE}
library(ncdf4)
# Downloaded August 25, 2017 from http://cse.ffpri.affrc.go.jp/shojih/data/index.html
ncfiles <- c("~/Data/Hashimoto/RH_yr_Hashimoto2015.nc",
             "~/Data/Hashimoto/RS_yr_Hashimoto2015.nc")

nc <- nc_open(ncfiles[1])
# These annual data start in 1901; extract 1990-2012
co2 <- ncvar_get(nc, "co2", start = c(1, 1, 1, 90), count = c(-1, -1, 1, 23))
nc_close(nc)
#co2 <- co2[400:600, 220:360,]  # punch a hole for testing: North America
co2 <- co2[500:600, 320:360,]  # punch a hole for testing: part of North America

do_fitting <- function(co2) {
  
  f <- function(rh) { 
    df <- data.frame(x = seq_along(rh), y = rh)
    tryCatch(lm(y ~ x, data = df), error = function(e) NA)
  }
  
  # Fit linear model to each grid cell (this is slow)
  mods <- apply(co2, c(1, 2), FUN = f)  # slow
  
  # Extract slopes
  slopes <- apply(mods, c(1, 2), FUN = function(x) 
    if(!is.na(x)) x[[1]]$coefficients[["x"]] else NA)
  slopes <- matrix(slopes, nrow = nrow(mods), ncol = ncol(mods))
  
  # Extract slope p-values
  signif <- apply(mods, c(1, 2), FUN = function(x) 
    if(!is.na(x)) summary(x[[1]])$coefficients["x", "Pr(>|t|)"] else NA)
  signif <- matrix(signif, nrow = nrow(mods), ncol = ncol(mods))
  
  return(list(slopes = slopes, signif = signif))
}

out <- do_fitting(co2)

image(out$slopes)
image(out$signif)
hist(out$signif)

ncells <- sum(!is.na(out$slopes))
pos_slope <- sum(out$slopes > 0, na.rm = TRUE)
signif_pos_slope <- sum(out$slopes > 0 & out$signif < 0.05, na.rm = TRUE)

```

Total cells = `r ncells`.

Cells with positive slope = `r pos_slope` or `r round(pos_slope / ncells * 100, 0)`%.

Cells with _significant_ positive slope = `r signif_pos_slope` or `r round(signif_pos_slope / ncells * 100, 0)`%.

### Re-do analysis with assumed error rate

```{r fuzz, echo=FALSE}

co2_fuzz <- co2 * rnorm(length(co2), mean = 1, sd = median_error)

out <- do_fitting(co2_fuzz)

image(out$slopes)
image(out$signif)
hist(out$signif)

ncells <- sum(!is.na(out$slopes))
pos_slope <- sum(out$slopes > 0, na.rm = TRUE)
signif_pos_slope <- sum(out$slopes > 0 & out$signif < 0.05, na.rm = TRUE)

```

Total cells = `r ncells`.

Cells with _significant_ positive slope (observations with `r round(median_error * 100, 0)`% measurement error) = `r signif_pos_slope` or `r round(signif_pos_slope / ncells * 100, 0)`%.

# Next steps

Next: make a nice graph of change over time 
 using a subset of data for readability
 Convert array to data frame and plot rs versus time
 with a line for each grid cell
